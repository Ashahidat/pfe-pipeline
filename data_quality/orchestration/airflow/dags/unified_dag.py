# airflow_dag_modular.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import os
import json
import sys
import importlib
from pyspark.sql import SparkSession


# Ajouter le chemin racine du projet pour trouver les validators
sys.path.append("/home/ashahi/PFE/pip/data_quality/orchestration")
sys.path.append("/home/ashahi/PFE/pip/data_quality")
from utils.db_utils import save_results_to_postgres
# from utils.atlas_entities import get_or_create_dataset, get_or_create_column


def run_modular_validations(**kwargs):
    dag_run = kwargs.get("dag_run")
    conf = dag_run.conf or {}

    file_path = conf.get("file_path")
    rules = conf.get("rules", {})

    if not file_path or not rules:
        raise ValueError(f"Param√®tres manquants : file_path={file_path}, rules={rules}")

    # Initialisation Spark
    spark = SparkSession.builder \
        .master("local[*]") \
        .appName("ModularValidation") \
        .config("spark.jars.packages", "com.amazon.deequ:deequ:2.0.7-spark-3.3") \
        .getOrCreate()

    # Lecture du fichier
    if file_path.endswith(".csv"):
        df = spark.read.option("header", "true").csv(file_path)
    else:
        df = spark.read.json(file_path)

    results = {}

    # Ex√©cution dynamique des validateurs
    for validation_type, validation_rules in rules.items():
        try:
            module_name = f"validators.{validation_type}_validator"
            module = importlib.import_module(module_name)
            print(f"üöÄ Ex√©cution du validator '{validation_type}'")

            # Certains modules ont besoin de spark, d'autres non
            if hasattr(module, "run") and callable(module.run):
                # Essayons de passer spark si n√©cessaire
                try:
                    results[validation_type] = module.run(spark, df, validation_rules)
                except TypeError:
                    results[validation_type] = module.run(df, validation_rules)
            print(f"‚úÖ Validator '{validation_type}' termin√©")
        except ModuleNotFoundError:
            print(f"‚ö†Ô∏è Module '{module_name}' introuvable. Ignor√©.")
        except Exception as e:
            print(f"‚ùå Erreur dans '{validation_type}': {e}")
            results[validation_type] = {"error": str(e)}

    # Sauvegarde des r√©sultats
    output_dir = "/home/ashahi/PFE/pip/data_quality/results"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "validation.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # üîπ Sauvegarde Postgres
    dag_run_id = kwargs.get("run_id")
    save_results_to_postgres(results, dag_run_id)


    spark.stop()
    return results

# D√©finition du DAG unifi√© modulaire
with DAG(
    dag_id="modular_validation_dag",
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False,
    tags=["data_quality", "modular"]
) as dag:

    run_validations = PythonOperator(
        task_id="run_modular_validations",
        python_callable=run_modular_validations
    )
